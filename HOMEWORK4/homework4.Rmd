---
title: "Homework4"
author: "Carlotta Porcelli - exam_ID 62"
date: "6/13/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PART 1

## Question 1A: Merge the ChIP peaks that overlap over 1bp or more. How many merged regions are produced compared to how many peaks you started with?

In order to run the _bedtools merge_, the _txnCHIP.bed_ file needs to be sorted. This is done by chromosome and then by start position.

```{bash, echo=TRUE, eval=FALSE}
sort -k1,1 -k2,2n txnChIP.bed > sorted_txnCHIP.bed # presort of .bed file
```

```{bash, echo=TRUE, eval=FALSE}
wc -l txnChIP.bed 
```
The length of the _txnCHIP.bed_ file is: 4380444.

Merging the _txnCHIP.bed_ file is done not only merging the intervals but also reporting the number of intervals that were integrated into the new file using the __-c 1__ (applying _option_ on first column) and __-o count__ parameters. 
```{bash, echo=TRUE, eval=FALSE}
bedtools merge -i sorted_txnCHIP.bed -c 1 -o count > count_merged_sorted_txnCHIP.bed
```

```{bash, echo=TRUE, eval=FALSE}
wc -l count_merged_sorted_txnCHIP.bed
```
The merge tool produced 746610 regions, with a difference of 3633834 regions compared to the initial peaks number. 

## Question 1B: Plot the distribution of ChIP peaks in each merged region. Briefly comment your plot.
```{r, echo=TRUE, warning=FALSE}
library("ggplot2")
CHIP_peaks <- read.table("count_merged_sorted_txnCHIP.bed", header = F)
CHIP_distribution <- ggplot(data=CHIP_peaks, aes(V4)) + 
            geom_bar(stat="bin", color='black', fill='grey', binwidth = 0.2) +
            scale_x_continuous(trans='log2') +
            xlab("log2-scaled merged regions") +
            ylab("Counts") +
            ggtitle("Distribution of CHIP peaks in merged regions") +
            theme(plot.title = element_text(hjust = 0.5))
CHIP_distribution
```

The plot shows that more than 3e+05 of the merged regions are composed of only one site. The number of merged regions decrease drastically with the increase of overlapping regions.

## Question 1C: Using R, produce a new BED file that contains the 10 merged regions having the highest number of ChIP peaks.
```{r, echo=TRUE}
top_10 <- as.data.frame(CHIP_peaks[order(-CHIP_peaks$V4),][0:10,])
write.table(top_10, file='top_10_CHIP_peaks.bed', quote = F, row.names = F, col.names = F)
```

## Question 1D: Upload this into the UCSC browser. Look at each merged region and try to interpret it, also taking the peaks it contains into account. What do the merged regions typically overlap? Is there a particular factor that is responsible for the clusters?
Once the top_10 CHIP peaks BED file has been uploaded on the UCSC browser, the track has been mapped to the hg19 assembly.
![Screenshots of the 10 merged regions with the highest number of ChIP peaks]

All of the merged regions overlap the RNA polymerase II subunit __POLR2A__. The __POLR2A__ forms the largest subunit of RNA polymerase II, enzyme responsible for synthesizing messenger RNA in eukaryotes. In addition, this subunit forms the DNA binding domain of the polymerase, a groove in which the DNA template is transcribed into RNA.[Reference: ](https://www.ncbi.nlm.nih.gov/gene/5430). The transcription factors binding sites are clustered because of the presence of __POLR2A__ and its repeatedly presence. 

## Question 1E: What could we have done to improve the analysis? 
To improve the analysis the merge function could be set to combine regions that overlap for more than one basepair. This constraint would produce less overlapping regions. 

# PART 2

## Question 2.1: Provide one line of code which will make symbolic links only to the 6 fastq files.
```{bash, eval=F, echo=T}
ln -s /home/bohta/HW4/part2/*.fastq /home/qbp693/
```

## Question 2.2: Check how well the sequencing run went - Use the 'wc' function to calculate the number of reads in all fastq files and comment on the results.
```{bash, eval=F, echo=T}
grep '^+$' -c *.fastq # counts the number of lines starting and ending with '+' sign
```
The count of reads for each file is: WT1_R1.fastq:45626717, WT1_R2.fastq:45626717, WT2_R1.fastq:41428670, WT2_R2.fastq:41428670, WT3_R1.fastq:19326183, WT3_R2.fastq:19326183
It is noticeable that there is no difference in number of reads between the strands R1 and R2 of the same library. This is because the two files come from the same cDNA sequence and they have probably been trimmed before removing the orphan reads.

## Question 2.3: 
### A) Report the command for running Kallisto on the WT1 RNA-seq data.
```{bash, echo=T, eval=F}
nice /home/bohta/bin/kallisto quant --index /home/bohta/HW4/part2/kallistoIndex 
    --fr-stranded -t 6 --plaintext --bias -o kallisto_out/ WT1_R1.fastq WT1_R2.fastq
```

### B) Report the number of pseudo-aligned reads.
The number of pseudo-aligned reads is 23720001.

### C*) Report the estimated average fragment length. Based on this result, what is then the distance between the 3' ends of the two reads in an average read pair?
The estimated average fragment length is 178.486. 
```{bash, echo=T, eval=FALSE}
cat WT1_R1.fastq | awk '{if(NR%4==2) print length($1)}' > input_readslength_R1.txt
cat WT1_R2.fastq | awk '{if(NR%4==2) print length($1)}' > input_readslength_R2.txt
```

```{r, echo=TRUE, eval=FALSE}
read_len_R1 <- read.table('input_readslength_R1.txt', header=F) # R1 reads length
read_len_R2 <- read.table('input_readslength_R2.txt', header=F) # R2 reads length
r1_mean <- mean(read_len_R1$V1) # average length of R1 reads
r2_mean <- mean(read_len_R2$V1) # average length of R2 reads
avg_fragment_length <- 178.486 
abs(avg_fragment_length - (r1_mean+r2_mean)) # computes the absolute distance between 3' ends
```
The resulting fragment from the computation is a transcript compatible with the mapped reads, this means that the distance between the two 3' ends of a read pair can be computed as the absolute difference between the average_fragment_length and the average length of the reads in R1 and R2.
This results in an average distance between the 3' ends of an average read pair of almost 18 bases.

## Question 2.4: 
### A*) Report the command for running Salmon on the WT1 RNA-seq data.
```{bash, eval=F, echo=T}
nice /home/bohta/bin/salmon quant --index /home/bohta/HW4/part2/salmonIndex -p 6 
  --libType A --seqBias --gcBias -1 WT1_R1.fastq -2 WT1_R2.fastq -o salmon_out/
```

### B) Report the most likely library type as identify by Salmon.
The automatically detected most likely library is ISF.

### C) Report mapping rate.
The mapping rate = 60.9567%.

## Question 2.5: Which tool aligned more reads? Comment on the result.
Salmon aligned 27812554 reads, with almost 61% of mapped reads whereas Kallisto mapped almost 52% . Kallisto is based on pseudoalignments to _compatible transcripts_, Salmon is based on lightweight alignments, chains of maximal and super maximal exact matches. Both of the tools mapped above 50% which can be considerated as a good result but surely Salmon performed better.

## Question 2.6: 
### A) Compare the estimated 'effective length' of the isoform 'TCONS_00000020' from Kallisto and Salmon to each other and the reference length. 
```{r, echo=TRUE, eval=T}
kallisto_quant <- read.table('abundance.tsv', header=T)
salmon_quant <- read.table('quant.sf', header=T)
e_len_kal <- kallisto_quant[kallisto_quant$target_id=='TCONS_00000020',]
e_len_salmon <- salmon_quant[salmon_quant$Name == 'TCONS_00000020',]
e_len_kal
e_len_salmon
```


### B) What could explain the difference in the effective length? Which estimate do you trust more?

The effective lengths will be affected by the estimated empirical fragment length distribution, the method of calculating effective lengths and whether or not bias correction is used. The most trustworthy is the Salmon computation because it corrects not only the for sequence-specific biases but also for the fragment-level GC biases. Moreover it is unlikely that the effective length is larger than the actual length as Kallisto shows.

# PART 3
## Question 3.1: Load the data into R. How many isoforms are quantified in the count data? 
```{r, echo=TRUE, eval=FALSE}
part3_data <- load('part3.Rdata')
nrow(countDF)
```
The number of isoforms quantified in the count data is: 5787.

## Question 3.2: Report the R code for how to calculate RPKM values.


## Question 3.3: Make a one-liner (without the use of ';') that outputs the mean RPKM value of each sample. The restrictions from the previous question no longer apply.

## Question 3.4: Make histograms of the distribution of the logRpkm expression values. 

## Question 3.5: For each tool use logRpkm to calculate all pairwise replicate Pearson correlations of the replicate expression values and report the numbers in a table (one table per tool). 

## Question 3.6: 
### A)	Construct a function which calculates the CV and use the apply() function to calculate the CV based on logRpkm values for Kallisto and Salmon (separately). 1p

### B)	Plot the distribution of CV values as density lines (in one single plot) using color to indicate the tool and log transform the x-axis (using log10). 1p

### C)	Comment on the CV plots using max 75 words. 2p


## Question 3.7: Based on all the results you have collected here (all of part 2 and all of part 3), discuss in max 100 words which tool would you choose to continue with if you wanted to make a differential expression analysis.


# PART 4
```{r, echo=TRUE, eval=TRUE}
set.seed(2017)
library(ggplot2)
library(GGally)
```

## Question 1: Read both the expression matrix ("ExpressionMatrix.tab") and study design ("StudyDesign.tab") into R. Report the number of samples and the number of genes.
```{r, echo=TRUE, eval=TRUE}
expression_m <- read.table('ExpressionMatrix.tab', header=T)
# head(expression_m)
study_design <- read.table('StudyDesign.tab', header=T)
# head(study_design)
samples_number <- ncol(expression_m)
samples_number
gene_number <- nrow(expression_m)
gene_number
```
The samples number is 75 and the genes number is 10000.

## Question 2: Perform PCA on the samples and report the amount of variance contained in the first 5 principle components. Note: You should scale the expression values before the PCA. 
```{r, echo=TRUE, eval=TRUE}
expression_m_transposed <- t(expression_m)
pca_genes <- prcomp(expression_m_transposed, center = T, scale. = T)
top5 <- summary(pca_genes)
top5
```
The expression values have been scaled using center and scale arguments set to TRUE in order to normalize the data. The amount of variance in the first 5 principal components is: PC1:0.05092, PC2:0.03281,  PC3:0.02097, PC4:0.01877, PC5:0.01525.

## Question 3: Make a plot of PC1 vs PC2, where knock down efficiency is indicated by color and knock down target is indicated by shape. Comment on the plot. 
```{r, echo=TRUE, eval=TRUE}
summary(study_design)
gene_plot <- data.frame(pca_genes$x, study_design) 
qplot(PC1, PC2, color=KnockDownEfficiency, shape = KnockDownTarget, 
      data=gene_plot, main='PCA of first 2 principal components') + 
  geom_point(size=0.6)+
  theme(plot.title = element_text(hjust = 0.5))
    
```
The plot shows a cluster of control genes with knock down efficiency equals to zero. 

## Question 4: Perform a K-means clustering of the data, using k=3 and 10 random starting points. Visualize the clustering by making a plot of PC1 vs PC2, where the clusters are indicated by color. Briefly comment on how the clustering corresponds to the known knockdown targets, using a maximum of 75 words
```{r, echo=TRUE, eval=TRUE}
k3_means <- kmeans(expression_m_transposed, centers=3, nstart = 10) # 3-means with 10 starting points
gene_plot$cluster <- factor(k3_means$cluster) # adding clusters to data frame
ggplot(data=gene_plot, aes(x=PC1, y=PC2, color=cluster)) + # plotting results
  geom_point(size=1) +
  ggtitle('PC1 and PC2 3k-means clustered of gene expression') +
  theme(plot.title = element_text(hjust = 0.5))
```


## Question 5: 

## Question 6: 


